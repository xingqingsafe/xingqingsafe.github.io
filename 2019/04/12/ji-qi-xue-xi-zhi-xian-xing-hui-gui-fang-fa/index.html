<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="机器学习之线性回归方法"><meta name="keywords" content="机器学习"><meta name="author" content="starjian"><meta name="copyright" content="starjian"><title>机器学习之线性回归方法 | 星晴</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-多元线性回归"><span class="toc-number">1.</span> <span class="toc-text">1. 多元线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-参数x0到xn的求解"><span class="toc-number">2.</span> <span class="toc-text">2.参数x0到xn的求解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-梯度下降法"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-利用归一化方程"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 利用归一化方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-两种求解方法的比较"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 两种求解方法的比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-回归方程和回归参数的显著性检验"><span class="toc-number">3.</span> <span class="toc-text">3.回归方程和回归参数的显著性检验</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">starjian</div><div class="author-info__description text-center">白帽子星晴的成长记录博客</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">36</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">13</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">星晴</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">机器学习之线性回归方法</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-04-12</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="1-多元线性回归"><a href="#1-多元线性回归" class="headerlink" title="1. 多元线性回归"></a>1. 多元线性回归</h2><p>以预测房子价格为例，房子的属性除了其面积大小外，这里还有房子楼层数、房子年龄、厅室数等。也就是说一个房子的特征将不只一个，而是多个。这里用<code>n</code>表示特征数目，<code>x(i)</code>表示的是训练集中第<code>i</code>个样本的特征向量，<code>x(i)j</code>表示的第<code>i</code>个样本的第<code>j</code>个特征值,同时为每个记录增加一个属性<code>x0=1</code>.对于这个预测房价的问题我们首先要估计未知参数<code>x0......xn</code>的值，为此我们进行<code>n</code>次独立观测，得到n组数据样本，将它们带入方程可以得到一个方程组，我们将其矩阵化表示。</p>
<h2 id="2-参数x0到xn的求解"><a href="#2-参数x0到xn的求解" class="headerlink" title="2.参数x0到xn的求解"></a>2.参数x0到xn的求解</h2><p>我们需要求得参数，需要使用参数的最小二乘估计，我们需要选择<code>x0-xn</code>的参数使残差平方和得到最小</p>
<p>残差平方和就是，实际值减去预测值的平方。</p>
<h3 id="2-1-梯度下降法"><a href="#2-1-梯度下降法" class="headerlink" title="2.1 梯度下降法"></a>2.1 梯度下降法</h3><p>梯度下降法：就是将残差平方和对每一个特征进行偏导，这个得到的倒数可以类似于一坡度，我们需要不断的根据这点的坡度逼近最低点，需要不停的进行迭代。在梯度下降之前，有时候数据不同特征之间存在很大的差距，有的范围在<code>1000-2000</code>之间，而有的仅在<code>0-10之</code>间，这时候就会使得大数吃小数的情况，同时使得数据不便处理，为了使得数据的范围大致相同，我们需要对数据进行归一化。目前有三种归一化方法：</p>
<p>1)<code>mapminmax</code>，最大最小化方法，缺点：当新数据加入的时候，可能导致<code>max</code>和<code>min</code>发生变化，需要重新定义。</p>
<p>2)<code>log</code>函数转换</p>
<p>3)<code>z-score</code> <code>normalization</code>:标准差标准化</p>
<p>算法原理：根据刚才得到的残差平方表示的方程组，对每一个特征的参数进行偏导，然后根据偏导不断迭代最终逼近残差最小的值，得到参数的估计值。</p>
<h3 id="2-2-利用归一化方程"><a href="#2-2-利用归一化方程" class="headerlink" title="2.2 利用归一化方程"></a>2.2 利用归一化方程</h3><p>刚才我们得到了残差平方和的式子，利用微积分的极值法，我们对每一个参数进行偏导使得等于0，得到一组方程组，将其表示成矩阵形式，移项等矩阵运算就得到归一化方程。</p>
<h3 id="2-3-两种求解方法的比较"><a href="#2-3-两种求解方法的比较" class="headerlink" title="2.3 两种求解方法的比较"></a>2.3 两种求解方法的比较</h3><p>1)梯度下降求解法：</p>
<p>优点：当<code>Features</code>数量很大的时候，就是特征数量很大的时候，梯度下降比归一化方程求解好。</p>
<p>缺点：需要将数据预处理，进行归一化，还要初始化学习速率，比较麻烦。</p>
<p>2)归一化方程：</p>
<p>优点：比较快速同时比较方便</p>
<p>缺点：当特征值特别多的时候比较慢，同时如果特征值之间存在多重共线性，导致x的转置乘以x将会没有逆矩阵，计算不出参数的估计值。</p>
<h2 id="3-回归方程和回归参数的显著性检验"><a href="#3-回归方程和回归参数的显著性检验" class="headerlink" title="3.回归方程和回归参数的显著性检验"></a>3.回归方程和回归参数的显著性检验</h2><p>我们通过上面建立的回归模型到底因变量和自变量是否存在线性关系？这个还需要们对其进行假设检验进行验证。</p>
<p>总离差平方和<code>SST</code>：数据的真实值<code>y</code>减去真实值的平均值的平方。反映了数据的波动性大小。<br>残差平方和<code>SSE</code>：真实值减去观测值的平方。 反映了除去<code>y</code>与<code>x</code>之间的线性关系之外的因素引起的数据y的波动。<br>回归平方和<code>SSR</code>：预测值减去真实值的平均值的平方。反映了线性拟合值与它们平均值的总偏差。</p>
<p>利用代数运算和正规方程组得到<code>SST=SSR+SSE</code>.所以<code>SSR</code>越大，说明由回归关系得到的观测值的波动性比例越大，就是说<code>y</code>与<code>x</code>的线性关系越显著，拟合效果越好。自由度相应的也需要进行分解，<code>n-1=(n-p-1)+p</code>最后基于自由度的分解，我们建立方差分析表。<code>SSR</code>均方差等于<code>MSR=SSR/P</code>,<code>MSE=SSE/(N-P-1)</code>,<code>F=MSR/MSE</code>。当假设<code>y</code>与<code>x1</code>,<code>x2</code>,…<code>xn</code>不存在线性关系时候，<code>F</code>服从于<code>F（p，n-p-1）</code>,尽管已经说明了<code>x</code>与<code>y</code>存在了线性关系，但是每一个特征值<code>x</code>都对<code>y</code>存在显著的影响吗？</p>
<p>所以我们还需要对其进行单个参数的假设检验。接下来我们需要一个拟合优度，它对于衡量模型对样本观测值的拟合程度，在总的离差平方和中，回归平方和占得比例越大，说明拟合效果越来越好，于是通过回归平方和与离差平方和的比例作为判优的标准。它越接近<code>1</code>，说明模型的拟合优度越高。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">starjian</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/04/12/ji-qi-xue-xi-zhi-xian-xing-hui-gui-fang-fa/">http://yoursite.com/2019/04/12/ji-qi-xue-xi-zhi-xian-xing-hui-gui-fang-fa/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/机器学习/">机器学习</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/04/12/python-pi-liang-sao-miao-phpmyadmin-ruo-kou-ling/"><i class="fa fa-chevron-left">  </i><span>python批量爆破phpMyadmin</span></a></div><div class="next-post pull-right"><a href="/2019/04/12/python-zi-dong-deng-lu-xiao-yuan-wang/"><span>python自动登陆校园网</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By starjian</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>